import re
import pandas as pd
from langchain_community.vectorstores import Chroma
import chromadb
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from nltk.corpus import stopwords
import nltk
from nltk.util import ngrams
import tkinter as tk
from tkinter import messagebox
import os
import sys

def resource_path(relative_path):
    if hasattr(sys, '_MEIPASS'):
        return os.path.join(sys._MEIPASS, relative_path)
    return os.path.join(os.path.dirname(os.path.abspath(__file__)), relative_path)

nltk.download('stopwords', quiet=True)

pdf_path = resource_path("Viena_classification.pdf")
loader = PyPDFLoader(pdf_path)
documents = loader.load()

paragrafs = []
for doc in documents:
    for block in doc.page_content.split(" \n \n"): # Easy separator at the time (I had to recreate the whole document)
        block = block.strip()
        if bloco:
            paragrafs.append(block)

clean_paragrafs = []
for p in paragrafos:
    p = re.sub(r"[ \xa0]+", " ", p)
    p = re.sub(r"\s+", " ", p)
    clean_paragrafs.append(p.strip())

pattern = re.compile(
    r"^(?P<classe>(?:\*?\s*A?\s*)?\d+(?:\.\d+){0,2})\s*-?\s*(?P<descricao>.+)",
    re.DOTALL
)

entries = []
for block in clean_paragrafs:
    match = pattern.match(block)
    if match:
        class = match.group("classe").strip()
        description = match.group("descricao").strip()
        if (
            len(description) < 3
            or "CLASSIFICAÇÃO" in descricao.upper()
            or "EDIÇÃO" in descricao.upper()
            or "LISTA" in descricao.upper()
        ):
            continue
        entries.append({
            "class": class,
            "description": description,
            "original_desc": block
        })

chroma_client = chromadb.EphemeralClient()

df = pd.DataFrame(entries).drop_duplicates()
embedding_model = HuggingFaceEmbeddings(model_name=resource_path("models/all-MiniLM-L6-v2")) # Model used
metadata = df[["class", "original_desc"]].to_dict(orient="records")

vectorstore = Chroma.from_texts(df["description"].tolist(), 
                                embedding_model, 
                                metadatas=metadata, 
                                client=chroma_client)

stopwords_pt = set(stopwords.words('portuguese'))

def clean_text(text):
    words = [w.lower() for w in re.findall(r'\w+', text) if w.lower() not in stopwords_pt]
    bi_grams = [' '.join(b) for b in ngrams(words, 2)]
    tri_grams = [' '.join(b) for b in ngrams(words, 3)]
    return set(words + bi_grams + tri_grams)

similarity = 0.75

def search_results(query):
    words = clean_text(query)
    results_per_word = []

    for word in words:
        res = vectorstore.similarity_search_with_score(word, k=25) # Only semantic search (the sintax search was not benefic in this task)
        for doc, score in res:
            if score <= similarity:
                results_per_word.append({
                    "class": doc.metadata['class'],
                    "text": doc.page_content.strip(),
                    "query_word": word,
                    "score": round(score, 3)
                })

    results_df = pd.DataFrame(results_per_word)
    if results_df.empty:
        return pd.DataFrame()
    organized_df = results_df.sort_values('score').drop_duplicates(subset='texto')
    return organized_df

def execute_search():
    query = input_query.get("1.0", "end-1c").strip()
    if not query:
        messagebox.showwarning("Warning!", "Please, write a description of your product or brand.")
        return

    resultados = search results(query)

    output_dir = os.path.dirname(sys.executable)
    output_path = os.path.join(output_dir, "results.txt")
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(f"Query: {query}\n\n\n")
        if not results.empty:
            f.write("Results with score <= 0.75:\n\n")
            for i, r in enumerate(results.to_dict(orient="records"), start=1):
                f.write(f"--- Result {i} ---\n")
                f.write(f"Query word: {r['query_word']}\n")
                f.write(f"Score: {r['score']:.3f}\n")
                f.write(f"Class: {r['class']}\n")
                f.write(f"Description: {r['text']}\n\n")
        else:
            f.write("There is no similar class for this description.\n")

    messagebox.showinfo("Sucess", "the file 'results.txt' was generated sucessfully!")
    input_query.delete("1.0", "end") 

def close_program():
    if messagebox.askyesno("Exit", "Are you sure?"):
        window.destroy()
        sys.exit(0)

window = tk.Tk()
window.title("LLM4trademark - Viena")
window.geometry("500x300")
window.protocol("WM_DELETE_WINDOW", close_program)

label = tk.Label(window, text="Describe your product or brand:", font=("Arial", 12))
label.pack(pady=10)

input_query = tk.Text(window, height=4, width=50, font=("Arial", 11))
input_query.pack(pady=5)

button = tk.Button(window, text="Search", command=execute_search,
                  font=("Arial", 12), bg="#0078D7", fg="white")
button.pack(pady=10)

exit_button = tk.Button(window, text="Exit", command=close_program,
                       font=("Arial", 11), bg="#D9534F", fg="white")
exit_button.pack(pady=5)

window.mainloop()

